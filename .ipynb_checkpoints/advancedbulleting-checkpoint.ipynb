{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from google.cloud import vision\n",
    "from google.cloud.vision import types\n",
    "\n",
    "from tqdm import tqdm\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shashvat/anaconda3/lib/python3.7/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = vision.ImageAnnotatorClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(filein):\n",
    "    with io.open(filein, 'rb') as image_file:\n",
    "        image = types.Image(content = image_file.read())\n",
    "    response = client.document_text_detection(image=image)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_list = []\n",
    "for i in range(10):\n",
    "    filein = \"./convert/7.012noteslindrew-{}.png\".format(i)\n",
    "    response_list.append(get_response(filein))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "special = \"&%$#_{}~^\\\\\"\n",
    "special_map = {}\n",
    "for s in special:\n",
    "    special_map[s] = \"\\\\\" + s\n",
    "special_map[\"~\"] = \"\\\\textasciitilde\"\n",
    "special_map[\"^\"] = \"\\\\textasciicircum\"\n",
    "special_map[\"\\\\\"] = \"\\\\textbackslash\"\n",
    "special_map[\"[\"] = \"{[}\"\n",
    "special_map[\"]\"] = \"{]}\"\n",
    "\n",
    "def parse_sym(s):\n",
    "    if s in special_map.keys():\n",
    "        return special_map[s]\n",
    "    return s\n",
    "\n",
    "def parse_detected_break(text, detected_break, in_itemize = False):\n",
    "    break_text = \"\"\n",
    "    kind = detected_break.type\n",
    "    \n",
    "    if kind:\n",
    "        if kind == 1:\n",
    "            break_text = \" \"\n",
    "        elif kind == 2:\n",
    "            break_text = \"    \"\n",
    "        elif kind == 3:\n",
    "            if in_itemize:\n",
    "                break_text = \"\\n\"\n",
    "            else:\n",
    "                break_text = \"\\\\\\\\\\n\"\n",
    "#             break_text = \"$3\\n\"\n",
    "        elif kind == 5:\n",
    "            if in_itemize:\n",
    "                break_text = \"\\n\"\n",
    "            else:\n",
    "                break_text = \"\\\\\\\\\\n\"\n",
    "#             break_text = \"$5\\n\"\n",
    "#         elif kind == 2:\n",
    "#             break_text = \" \"\n",
    "        \n",
    "#         elif \n",
    "\n",
    "    if detected_break.is_prefix:\n",
    "        return break_text + text\n",
    "    else:\n",
    "        return text + break_text\n",
    "\n",
    "def avg_sym_width(block):\n",
    "    widths = []\n",
    "    for paragraph in block.paragraphs:\n",
    "        for word in paragraph.words:\n",
    "            for sym in word.symbols:\n",
    "                widths.append(sym.bounding_box.vertices[1].x - sym.bounding_box.vertices[0].x)\n",
    "    return statistics.median(widths)\n",
    "\n",
    "def avg_sym_heights(document):\n",
    "    widths = []\n",
    "    for page in document.pages:\n",
    "        for block in page.blocks:\n",
    "            for paragraph in block.paragraphs:\n",
    "                for word in paragraph.words:\n",
    "                    for sym in word.symbols:\n",
    "                        widths.append(sym.bounding_box.vertices[2].y - sym.bounding_box.vertices[0].y)\n",
    "    return statistics.median(widths)\n",
    "\n",
    "def avg_sym_height_block(block):\n",
    "    widths = []\n",
    "    for paragraph in block.paragraphs:\n",
    "        for word in paragraph.words:\n",
    "            for sym in word.symbols:\n",
    "                widths.append(sym.bounding_box.vertices[2].y - sym.bounding_box.vertices[0].y)\n",
    "    return statistics.median(widths)\n",
    "\n",
    "def get_raw(block):\n",
    "    b = \"\"\n",
    "    for p in block.paragraphs:\n",
    "        for w in p.words:\n",
    "            for s in w.symbols:\n",
    "                b += s.text\n",
    "    return b\n",
    "\n",
    "def extract_block(block):\n",
    "    avg_width = avg_sym_width(block)\n",
    "    \n",
    "    b = \"\"\n",
    "    \n",
    "    in_itemize = False\n",
    "    itemize_levels = 0\n",
    "    last_itemize = None\n",
    "    \n",
    "    for paragraph in block.paragraphs:\n",
    "        p = \"\"\n",
    "        for word in paragraph.words:\n",
    "            w = \"\"\n",
    "            for symbol in word.symbols:\n",
    "#                 print(symbol.property.detected_languages)\n",
    "                detected_break = symbol.property.detected_break\n",
    "                text = parse_sym(symbol.text)\n",
    "                \n",
    "                if text == \".\" and (p == \"1\" or p == \"|\"):\n",
    "                    p = \"\"\n",
    "                    text = \"•\"\n",
    "                elif len(p) >= 2 and p[-2:] == \"\\n1\":\n",
    "                    text = \"•\"\n",
    "                    p = p[:-1]\n",
    "                    \n",
    "                washere = False\n",
    "                if text in \"•-\" and (p == \"\" or p[-1] == '\\n'):\n",
    "                    washere = True\n",
    "                    \n",
    "                    text = \"\"\n",
    "                    if not in_itemize:\n",
    "                        text += \"\\\\begin{itemize}\\n\"\n",
    "                        in_itemize = True\n",
    "                        itemize_levels += 1\n",
    "                    if in_itemize:\n",
    "                        text += \"\\\\item \"\n",
    "                        \n",
    "                    if last_itemize and itemize_levels <= 3:\n",
    "                        dist = symbol.bounding_box.vertices[0].x - last_itemize.bounding_box.vertices[0].x\n",
    "                        \n",
    "                        y1 = [last_itemize.bounding_box.vertices[0].y, last_itemize.bounding_box.vertices[2].y]\n",
    "                        y2 = [symbol.bounding_box.vertices[0].y, symbol.bounding_box.vertices[2].y]\n",
    "            \n",
    "                        inter = set(range(y1[0], y1[1])).intersection(set(range(y2[0], y2[1])))\n",
    "                        if dist >= 4 * avg_width:\n",
    "                            text = \"\\\\begin{itemize}\\n\" + text\n",
    "                            itemize_levels += 1\n",
    "                            in_itemize = True\n",
    "                        elif dist <= -4 * avg_width and itemize_levels >= 2:\n",
    "                            text = \"\\\\end{itemize}\\n\" + text\n",
    "                            itemize_levels -= 1\n",
    "                            in_itemize = (itemize_levels != 0)\n",
    "                \n",
    "                    last_itemize = symbol\n",
    "                w += parse_detected_break(text, detected_break, in_itemize)\n",
    "                \n",
    "            if in_itemize and \"\\\\item\" not in w:\n",
    "                if p != \"\" and p[-1] == '\\n':\n",
    "                    for _ in range(itemize_levels):\n",
    "                         p += \"\\\\end{itemize}\\n\"\n",
    "                    itemize_levels = 0\n",
    "                    in_itemize = False\n",
    "                    last_itemize = None\n",
    "                \n",
    "            p += parse_detected_break(w, word.property.detected_break, in_itemize)\n",
    "        \n",
    "        b += parse_detected_break(p, paragraph.property.detected_break, in_itemize)\n",
    "        \n",
    "    if in_itemize:\n",
    "        for _ in range(itemize_levels):\n",
    "             b += \"\\\\end{itemize}\\n\"\n",
    "        \n",
    "        itemize_levels = 0\n",
    "        in_itemize = False\n",
    "        last_itemize = None\n",
    "        \n",
    "    text = parse_detected_break(b, block.property.detected_break)\n",
    "    text = text.replace(\"\\\\\\\\\\\\begin{itemize}\", \"\\\\begin{itemize}\")\n",
    "    text = text.replace(\"\\\\\\\\\\n\\\\begin{itemize}\", \"\\n\\\\begin{itemize}\")\n",
    "#     text = text.replace(\"\\\\\\\\\\\\begin{itemize}\")\n",
    "#     text = text.replace(\"\\\\end{itemize}\\n\\\\begin{itemize}\", \"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(\"dev2take2.tex\", 'w') as f:\n",
    "    fmt = open(\"format.tex\", \"r\").read()\n",
    "    \n",
    "    al = \"\"\n",
    "    for response in response_list:\n",
    "        avg_height = avg_sym_heights(response.full_text_annotation)\n",
    "        for page in response.full_text_annotation.pages:\n",
    "            for block in page.blocks:\n",
    "                height = avg_sym_height_block(block)\n",
    "                if height >= 1.9 * avg_height:\n",
    "                    al += \"\\\\newpage \\n \\\\section{\" + get_raw(block) + \"}\"\n",
    "#                 print(avg_sym_width(block))\n",
    "                else:\n",
    "                    text = extract_block(block)\n",
    "                    al += text\n",
    "                \n",
    "    text = fmt + al + \"\\\\end{document}\"\n",
    "                \n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
